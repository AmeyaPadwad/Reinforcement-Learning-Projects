{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from numpy import average\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30700fb",
   "metadata": {},
   "source": [
    "#### Defining Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_STATE = (1, 6)\n",
    "START_STATE = (7, 2)\n",
    "ACTIONS = [\"up\", \"down\", \"left\", \"right\"]\n",
    "NUM_ACTIONS = len(ACTIONS)\n",
    "\n",
    "def init_env() -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to initialize the environment matrix\n",
    "    \"\"\"\n",
    "    # Environment matrix\n",
    "    # regular = 0    wall = 1    red = 2     yellow = 3    start = 4    end = 5\n",
    "    Env = np.zeros([10, 10])\n",
    "\n",
    "    # Defining border walls\n",
    "    Env[:, [0]] = np.ones([10, 1])\n",
    "    Env[:, [9]] = np.ones([10, 1])\n",
    "    Env[0, :] = np.ones([1, 10])\n",
    "    Env[9, :] = np.ones([1, 10])\n",
    "\n",
    "    # Defining cell properties inside the maze\n",
    "    # wall = 1\n",
    "    Env[2, 4] = 1\n",
    "    Env[3, 3:7] = np.ones([1, 4])\n",
    "    Env[3, 8] = 1\n",
    "    Env[4, 4] = 1\n",
    "    Env[5, 6] = 1\n",
    "    Env[6, 1:4] = np.ones([1, 3])\n",
    "    Env[6, 6] = 1\n",
    "    Env[7, 6] = 1\n",
    "    Env[8, 6] = 1\n",
    "\n",
    "    # red = 2\n",
    "    Env[3, 2] = 2\n",
    "    Env[6, 4] = 2\n",
    "    Env[7, 3] = 2\n",
    "    Env[7, 7] = 2\n",
    "\n",
    "    # yellow = 3\n",
    "    Env[1, 2] = 3\n",
    "    Env[3, 7] = 3\n",
    "    Env[5, 2] = 3\n",
    "    Env[5, 7] = 3\n",
    "\n",
    "    # start = 4\n",
    "    Env[START_STATE[0], START_STATE[1]] = 4\n",
    "\n",
    "    # goal = 5\n",
    "    Env[GOAL_STATE[0], GOAL_STATE[1]] = 5\n",
    "\n",
    "    return Env\n",
    "\n",
    "def get_all_states(Env):\n",
    "    \"\"\"\n",
    "    Function to get all usable states in the environment\n",
    "    \"\"\"\n",
    "    all_states = []\n",
    "    for i in range(Env.shape[0]):\n",
    "        for j in range(Env.shape[1]):\n",
    "            if Env[i, j] != 1:  # should not be a wall\n",
    "                all_states.append((i, j))\n",
    "    \n",
    "    return all_states\n",
    "\n",
    "def get_as_matrix(values, Env, type):\n",
    "    \"\"\"\n",
    "    Function to convert the policy dictionary to a matrix\n",
    "    \"\"\"\n",
    "    matrix = np.full(Env.shape, '', dtype=object)\n",
    "    all_states = get_all_states(Env)\n",
    "    i = 0\n",
    "    for state in all_states:\n",
    "        if type == \"policy\":\n",
    "            matrix[state[0], state[1]] = ACTIONS[values[i]]\n",
    "        elif type == \"q_values\":\n",
    "            matrix[state[0], state[1]] = str(np.round(values[i], 2))\n",
    "        i += 1\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def plot_env(Env, title, annot_matrix=False, show=True) -> None:\n",
    "    \"\"\"\n",
    "    Function to plot the environment matrix\n",
    "    \"\"\"\n",
    "    # Define colors\n",
    "    colors = {\n",
    "        0: [1, 1, 1],        # White\n",
    "        1: [0, 0, 0],        # Black\n",
    "        2: [0.55, 0, 0],     # Light Brown\n",
    "        3: [0.96, 0.8, 0.6], # Dark Red\n",
    "        4: [0, 0, 1],        # Green\n",
    "        5: [0, 1, 0]         # Blue\n",
    "    }\n",
    "\n",
    "\n",
    "    rgb_maze = np.zeros((Env.shape[0], Env.shape[1], 3))\n",
    "    for i in range(Env.shape[0]):\n",
    "        for j in range(Env.shape[1]):\n",
    "            rgb_maze[i, j] = colors.get(Env[i, j], [1, 1, 1])\n",
    "\n",
    "    if annot_matrix is not False:\n",
    "        annot_matrix = np.where(Env == 1, '', annot_matrix)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(Env,fmt=\"\",  cmap=sns.color_palette([colors[i] for i in range(6)]), cbar=False,annot=annot_matrix, linewidths=0.5, linecolor='black')\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "    plt.title(title)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_policy(Env, policy, title) -> None:\n",
    "    \"\"\"\n",
    "    Function to plot the optimal policy matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_env(Env, title, annot_matrix=False, show=False)\n",
    "    for i in range(Env.shape[0]):\n",
    "        for j in range(Env.shape[1]):\n",
    "            if policy[i, j] == \"up\":\n",
    "                plt.arrow(j+0.5, i+0.85, 0, -0.5, width=0.04, color='black')  # Up\n",
    "            if policy[i, j] == \"right\":\n",
    "                plt.arrow(j+0.15, i+0.5, 0.5, 0, width=0.04, color='black')  # Right\n",
    "            if policy[i, j] == \"down\":\n",
    "                plt.arrow(j+0.5, i+0.15, 0, 0.50, width=0.04, color='black')  # Down\n",
    "            if policy[i, j] == \"left\":\n",
    "                plt.arrow(j+0.85, i+0.5, -0.5, 0, width=0.04, color='black')  # Left\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_optimal_path(Env, path, title) -> None:\n",
    "    \"\"\"\n",
    "    Function to plot the optimal path\n",
    "    \"\"\"\n",
    "    plot_env(Env, title, annot_matrix=False, show=False)\n",
    "    \n",
    "    for state_cr, direction in path:\n",
    "        r = state_cr[0] # x_coordinate\n",
    "        c = state_cr[1] # y_coordinate\n",
    "\n",
    "        if direction == 'right':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0.8, 0, width=0.04, color='black')   # Right\n",
    "        if direction == 'left':\n",
    "            plt.arrow(c + 0.5, r + 0.5, -0.8, 0, width=0.04, color='black')  # Left\n",
    "        if direction == 'up':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0, -0.8, width=0.04, color='black')  # Up\n",
    "        if direction == 'down':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0, 0.8, width=0.04, color='black')  # Down\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_reward(next_state_type, wall_hit) -> float:\n",
    "    \"\"\"\n",
    "    Function to return the reward of an action based on next state and if wall was hit\n",
    "    \"\"\"\n",
    "    next_state_type = int(next_state_type)\n",
    "    reward = -1 # For taking an action\n",
    "\n",
    "    if wall_hit:         # Wall\n",
    "        reward += -0.8\n",
    "\n",
    "    if next_state_type == 2:  # Red\n",
    "        reward += -10\n",
    "    elif next_state_type == 3:  # Yellow\n",
    "        reward += -5\n",
    "    elif next_state_type == 5:  # Goal\n",
    "        reward += 100\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def get_next_state(Env, state, action) -> tuple:\n",
    "    \"\"\"\n",
    "    Function to return the next state given the current state and action\n",
    "    \"\"\"\n",
    "    i, j = state[0], state[1]\n",
    "    wall_hit = False\n",
    "\n",
    "    if action == \"up\":\n",
    "        next_i, next_j = i - 1, j\n",
    "    elif action == \"down\":\n",
    "        next_i, next_j = i + 1, j\n",
    "    elif action == \"left\":\n",
    "        next_i, next_j = i, j - 1\n",
    "    elif action == \"right\":\n",
    "        next_i, next_j = i, j + 1\n",
    "\n",
    "    if Env[next_i, next_j] == 1:  # Wall\n",
    "        next_i, next_j = i, j\n",
    "        wall_hit = True\n",
    "    \n",
    "    return next_i, next_j, wall_hit\n",
    "\n",
    "def get_probabilities(action, p) -> list:\n",
    "    \"\"\"\n",
    "    Function to return the probabilities of each action given the current action\n",
    "    \"\"\"\n",
    "    probabilities = {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 0}\n",
    "    action = list(probabilities.keys())[action]\n",
    "\n",
    "    if action == \"up\":\n",
    "        probabilities[\"up\"] = 1 - p\n",
    "        probabilities[\"down\"] = 0\n",
    "        probabilities[\"left\"] = p / 2\n",
    "        probabilities[\"right\"] = p / 2\n",
    "    elif action == \"down\":\n",
    "        probabilities[\"up\"] = 0\n",
    "        probabilities[\"down\"] = 1 - p\n",
    "        probabilities[\"left\"] = p / 2\n",
    "        probabilities[\"right\"] = p / 2\n",
    "    elif action == \"left\":\n",
    "        probabilities[\"up\"] = p / 2\n",
    "        probabilities[\"down\"] = p / 2\n",
    "        probabilities[\"left\"] = 1 - p\n",
    "        probabilities[\"right\"] = 0\n",
    "    elif action == \"right\":\n",
    "        probabilities[\"up\"] = p / 2\n",
    "        probabilities[\"down\"] = p / 2\n",
    "        probabilities[\"left\"] = 0\n",
    "        probabilities[\"right\"] = 1 - p\n",
    "    \n",
    "    return list(probabilities.values())\n",
    "\n",
    "def get_optimal_path(Env, start, end, optimal_policy):\n",
    "    \"\"\"\n",
    "    Function to get the optimal path from start to end\n",
    "    \"\"\"\n",
    "    curr_state = start\n",
    "    path = []\n",
    "    visited_states = []\n",
    "    while curr_state != end:\n",
    "        if curr_state in visited_states:\n",
    "            print(\"No optimal path found.\")\n",
    "            return path\n",
    "\n",
    "        visited_states.append(curr_state)\n",
    "        i, j = curr_state[0], curr_state[1]\n",
    "        action = optimal_policy[i, j]\n",
    "        path.append((curr_state, action))\n",
    "        next_i, next_j, wall_hit = get_next_state(Env, curr_state, action)\n",
    "        curr_state = (next_i, next_j)\n",
    "\n",
    "    return path\n",
    "\n",
    "def get_random_state(Env):\n",
    "    \"\"\"\n",
    "    Function to get a random state from the environment\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        i = random.randint(1, Env.shape[0] - 2)\n",
    "        j = random.randint(1, Env.shape[1] - 2)\n",
    "        if Env[i, j] != 1 and Env[i, j] != 5:  # should not be the goal state or a wall\n",
    "            break\n",
    "    \n",
    "    return (i, j)\n",
    "\n",
    "def calculate_epsilon(epsilon, ep_no):\n",
    "    \"\"\"\n",
    "    Function to calculate the epsilon value for epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    return max(0.1, epsilon ** ep_no)\n",
    "\n",
    "def calculate_moving_avg(metrics):\n",
    "    avg = []\n",
    "    for i in range(1, len(metrics)+1):\n",
    "        m = min(25, i)\n",
    "        avg.append(average(metrics[i-m:i]))\n",
    "\n",
    "    return avg\n",
    "\n",
    "def plot_avg(metrics, ylabel):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(metrics)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f\"No of episodes vs {ylabel}\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c4b21",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5295463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network (DQN) to approximate Q-values for each action given a state.\n",
    "    Architecture:\n",
    "    - Input Layer: state (x, y) → 2 nodes\n",
    "    - Hidden Layers: Fully connected with ReLU activations\n",
    "    - Output Layer: Q-values for all possible actions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DuelingQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Value stream (output = 1)\n",
    "        self.value_stream = nn.Linear(128, 1)\n",
    "\n",
    "        # Advantage stream (output = 4)\n",
    "        self.advantage_stream = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "\n",
    "        value = self.value_stream(features)\n",
    "        advantage = self.advantage_stream(features)\n",
    "\n",
    "        # Combine V(s) and A(s, a) to get Q(s, a)\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "\n",
    "        return q_values\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer to store past experiences (transitions)\n",
    "    Helps in breaking correlation between consecutive experiences.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store a transition tuple in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a random mini-batch from the buffer for training.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(torch.tensor, zip(*batch))\n",
    "        return state.float(), action.long(), reward.float(), next_state.float(), done.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent that interacts with environment, stores experiences, \n",
    "    learns from them, and updates the Q-network.\n",
    "    \"\"\"\n",
    "    def __init__(self, Env, lr, gamma, tau, buffer_size, batch_size, double_DQN = False, dueling_DQN = False):\n",
    "        self.states = torch.tensor(get_all_states(Env), dtype=torch.float32)\n",
    "        self.double_DQN = double_DQN\n",
    "\n",
    "        if dueling_DQN:\n",
    "            self.q_network = DuelingQNetwork()\n",
    "            self.target_network = DuelingQNetwork()\n",
    "        else:\n",
    "            self.q_network = QNetwork()\n",
    "            self.target_network = QNetwork()\n",
    "        \n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())  # Initial sync\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Selects an action using epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(NUM_ACTIONS)  # Random action\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        return self.q_network(state).argmax().item()\n",
    "\n",
    "    def get_policy(self):\n",
    "        \"\"\"\n",
    "        Returns the optimal policy derived from the Q-network.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(self.states)\n",
    "            return q_values.argmax(dim=1).numpy()\n",
    "\n",
    "    def get_q_values(self):\n",
    "        \"\"\"\n",
    "        Returns the Q-values for all actions given a state.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(self.states)\n",
    "        return q_values.numpy().flatten()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the Q-network using a batch of experiences from the replay buffer.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample a batch\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # Current Q-value for taken action\n",
    "        current_q_values = self.q_network(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Update main Q-network using target network\n",
    "        with torch.no_grad():\n",
    "            if self.double_DQN:\n",
    "                armgax_next_action = self.q_network(next_state).argmax(1)\n",
    "                next_q_values = self.target_network(next_state).gather(1, armgax_next_action.unsqueeze(1)).squeeze(1)\n",
    "                target_q_values = reward + self.gamma * next_q_values * (1 - done)\n",
    "\n",
    "            else:\n",
    "                max_next_q_values = self.target_network(next_state).max(1)[0]\n",
    "                target_q_values = reward + self.gamma * max_next_q_values * (1 - done)\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft update target network\n",
    "        for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "def simulate(Env, num_episodes, max_steps, buffer_size, lr, gamma, tau, batch_size, epsilon, p, steps_update, double_DQN = False, dueling_DQN = False):\n",
    "    agent = DQNAgent(Env, lr, gamma, tau, buffer_size, batch_size, double_DQN, dueling_DQN)\n",
    "    rewards, losses = [], []\n",
    "\n",
    "    for ep_no in range(num_episodes):\n",
    "        state = get_random_state(Env)\n",
    "        total_reward, total_loss = 0, 0\n",
    "        step = 0\n",
    "\n",
    "        while step < max_steps or state != GOAL_STATE:\n",
    "            step += 1\n",
    "            epsilon_value = calculate_epsilon(epsilon, step) # calculating epsilon value for this step\n",
    "            action = agent.select_action(state, epsilon_value) # select action using epsilon greedy policy\n",
    "            probabilities = get_probabilities(action, p) # get probabilities for each action since env is stochastic\n",
    "            action_taken = np.random.choice(ACTIONS, p=probabilities) # select actual action based on probabilities\n",
    "            next_i, next_j, wall_hit = get_next_state(Env, state, action_taken) # get next state and check if wall was hit\n",
    "            next_state = (next_i, next_j)\n",
    "            reward = get_reward(Env[next_state], wall_hit) # get reward based on next state and wall hit\n",
    "            if next_state == GOAL_STATE:\n",
    "                done = 1\n",
    "            else:\n",
    "                done = 0\n",
    "\n",
    "            # Store transition\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            # Train agent\n",
    "            if step % steps_update == 0:\n",
    "                loss = agent.train()\n",
    "                if loss:\n",
    "                    total_loss += loss\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "    optimal_policy = agent.get_policy()\n",
    "    q_values = agent.get_q_values()\n",
    "    optimal_policy_matrix = get_as_matrix(optimal_policy, Env, \"policy\")\n",
    "    q_values_matrix = get_as_matrix(q_values, Env, \"q_values\")\n",
    "    avg_rewards = calculate_moving_avg(rewards)\n",
    "    avg_losses = calculate_moving_avg(losses)\n",
    "\n",
    "    return optimal_policy_matrix, q_values_matrix, avg_rewards, avg_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN\n",
    "num_episodes = 500\n",
    "max_steps = 50\n",
    "buffer_size = 5000\n",
    "lr=1e-3\n",
    "gamma=0.99\n",
    "tau=1e-2\n",
    "batch_size=64\n",
    "epsilon=0.99\n",
    "p = 0.025\n",
    "steps_update = 1\n",
    "\n",
    "Env = init_env()\n",
    "optimal_policy, q_values, avg_rewards, avg_losses = simulate(Env, num_episodes, max_steps, buffer_size, lr, gamma, tau, batch_size, epsilon, p, steps_update)\n",
    "optimal_path = get_optimal_path(Env, START_STATE, GOAL_STATE, optimal_policy)\n",
    "plot_policy(Env, optimal_policy, \"Optimal Policy\")\n",
    "plot_optimal_path(Env, optimal_path, \"Optimal Path\")\n",
    "plot_avg(avg_rewards, \"Average Rewards\")\n",
    "plot_avg(avg_losses, \"Average Losses\")\n",
    "plot_env(Env, \"State Values\", q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17646954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN\n",
    "num_episodes = 500\n",
    "max_steps = 50\n",
    "buffer_size = 5000\n",
    "lr=1e-3\n",
    "gamma=0.99\n",
    "tau=1e-2\n",
    "batch_size=64\n",
    "epsilon=0.99\n",
    "p = 0.025\n",
    "steps_update = 1\n",
    "\n",
    "Env = init_env()\n",
    "optimal_policy, q_values, avg_rewards, avg_losses = simulate(Env, num_episodes, max_steps, buffer_size, lr, gamma, tau, batch_size, epsilon, p, steps_update, double_DQN=True)\n",
    "optimal_path = get_optimal_path(Env, START_STATE, GOAL_STATE, optimal_policy)\n",
    "plot_policy(Env, optimal_policy, \"Optimal Policy\")\n",
    "plot_optimal_path(Env, optimal_path, \"Optimal Path\")\n",
    "plot_avg(avg_rewards, \"Average Rewards\")\n",
    "plot_avg(avg_losses, \"Average Losses\")\n",
    "plot_env(Env, \"State Values\", q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dueling DQN\n",
    "num_episodes = 500\n",
    "max_steps = 50\n",
    "buffer_size = 5000\n",
    "lr=1e-3\n",
    "gamma=0.99\n",
    "tau=1e-2\n",
    "batch_size=64\n",
    "epsilon=0.99\n",
    "p = 0.025\n",
    "steps_update = 1\n",
    "\n",
    "Env = init_env()\n",
    "optimal_policy, q_values, avg_rewards, avg_losses = simulate(Env, num_episodes, max_steps, buffer_size, lr, gamma, tau, batch_size, epsilon, p, steps_update, dueling_DQN=True)\n",
    "optimal_path = get_optimal_path(Env, START_STATE, GOAL_STATE, optimal_policy)\n",
    "plot_policy(Env, optimal_policy, \"Optimal Policy\")\n",
    "plot_optimal_path(Env, optimal_path, \"Optimal Path\")\n",
    "plot_avg(avg_rewards, \"Average Rewards\")\n",
    "plot_avg(avg_losses, \"Average Losses\")\n",
    "plot_env(Env, \"State Values\", q_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
