{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_acc_rewards(run_acc_rewards, num_episodes, num_runs):\n",
    "    \"\"\"\n",
    "    Function to calculate the average accumulated rewards\n",
    "    \"\"\"\n",
    "    avg_acc_rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        avg_acc_reward = 0\n",
    "        for j in range(num_runs):\n",
    "            avg_acc_reward += run_acc_rewards[j][i]\n",
    "        avg_acc_reward /= num_runs\n",
    "        avg_acc_rewards.append(avg_acc_reward)\n",
    "\n",
    "    return avg_acc_rewards\n",
    "\n",
    "def plot_avg_acc_rewards(avg_acc_rewards, title):\n",
    "    \"\"\"\n",
    "    Function to plot the average accumulated rewards\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for k, v in avg_acc_rewards.items():\n",
    "        plt.plot(v)\n",
    "\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Average Accumulated Rewards\")\n",
    "    plt.title(title)\n",
    "    plt.legend(avg_acc_rewards.keys())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_STATE = (3, 13)\n",
    "START_STATE = (15, 4) \n",
    "\n",
    "def init_env() -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to initialize the environment matrix\n",
    "    \"\"\"\n",
    "    # Environment matrix\n",
    "    # regular = 0    wall = 1    oil = 2     bump = 3    start = 4    end = 5\n",
    "    Env = np.zeros([20, 20])\n",
    "\n",
    "    # Defining border walls\n",
    "    Env[:, [0]] = np.ones([20, 1])\n",
    "    Env[:, [19]] = np.ones([20, 1])\n",
    "    Env[0, :] = np.ones([1, 20])\n",
    "    Env[19, :] = np.ones([1, 20])\n",
    "\n",
    "    # Defining cell properties inside the maze\n",
    "    # wall = 1\n",
    "    Env[2, 5] = 1\n",
    "    Env[3, 5] = 1\n",
    "    Env[4, 3:17] = np.ones([1, 14])\n",
    "    Env[5, 3] = 1\n",
    "    Env[6, 3] = 1; Env[6, 6] = 1; Env[6, 9] = 1; Env[6, 15] = 1\n",
    "    Env[7, 3] = 1; Env[7, 6] = 1; Env[7, 9] = 1; Env[7, 12:16] = np.ones([1, 4])\n",
    "    Env[8, 6] = 1; Env[8, 9] = 1; Env[8, 15] = 1\n",
    "    Env[9, 6] = 1; Env[9, 9] = 1; Env[9, 15] = 1\n",
    "    Env[10, 1:5] = np.ones([1, 4]); Env[10, 6] = 1; Env[10, 9:11] = [1, 1]; Env[10, 15] = 1\n",
    "    Env[11, 6] = 1; Env[11, 10] = 1; Env[11, 13] = 1; Env[11, 15:18] = np.ones([1, 3])\n",
    "    Env[12, 3:8] = np.ones([1, 5]); Env[12, 10] = 1; Env[12, 13] = 1; Env[12, 17] = 1\n",
    "    Env[13, 7] = 1; Env[13, 10] = 1; Env[13, 13] = 1; Env[13, 17] = 1\n",
    "    Env[14, 7] = 1; Env[14, 10] = 1; Env[14, 13] = 1\n",
    "    Env[15, 7] = 1; Env[15, 13:17] = [1, 1, 1, 1]\n",
    "    Env[17, 1:3] = [1, 1]; Env[17, 7:13] = [1, 1, 1, 1, 1, 1]\n",
    "\n",
    "    # oil = 2\n",
    "    Env[2, 8] = 2; Env[2, 16] = 2\n",
    "    Env[4, 2] = 2\n",
    "    Env[5, 6] = 2\n",
    "    Env[10, 18] = 2\n",
    "    Env[14, 14] = 2\n",
    "    Env[15, 10] = 2\n",
    "    Env[16, 10] = 2\n",
    "    Env[17, 14] = 2; Env[17, 17] = 2\n",
    "    Env[18, 7] = 2\n",
    "\n",
    "    # bump = 3\n",
    "    Env[1, 11] = 3; Env[1, 12] = 3\n",
    "    Env[2, 1:4] = [3, 3, 3]\n",
    "    Env[5, 1] = 3; Env[5, 9] = 3; Env[5, 17] = 3\n",
    "    Env[6, 17] = 3\n",
    "    Env[7, 2] = 3; Env[7, 10:12] = [3, 3]; Env[7, 17] = 3\n",
    "    Env[8, 17] = 3\n",
    "    Env[12, 11:13] = [3, 3]\n",
    "    Env[14, 1:3] = [3, 3]\n",
    "    Env[15, 17:19] = [3, 3]\n",
    "    Env[16, 7] = 3\n",
    "\n",
    "    # start = 4\n",
    "    Env[15, 4] = 4\n",
    "\n",
    "    # goal = 5\n",
    "    Env[GOAL_STATE[0], GOAL_STATE[1]] = 5\n",
    "\n",
    "    return Env\n",
    "\n",
    "def plot_env(Env, title, annot_matrix=False, show=True) -> None:\n",
    "    \"\"\"\n",
    "    Function to plot the environment matrix\n",
    "    \"\"\"\n",
    "    # Define colors\n",
    "    colors = {\n",
    "        0: [1, 1, 1],        # White\n",
    "        1: [0, 0, 0],        # Black\n",
    "        2: [0.55, 0, 0],     # Light Brown\n",
    "        3: [0.96, 0.8, 0.6], # Dark Red\n",
    "        4: [0, 0, 1],        # Green\n",
    "        5: [0, 1, 0]         # Blue\n",
    "    }\n",
    "\n",
    "\n",
    "    rgb_maze = np.zeros((Env.shape[0], Env.shape[1], 3))\n",
    "    for i in range(Env.shape[0]):\n",
    "        for j in range(Env.shape[1]):\n",
    "            rgb_maze[i, j] = colors.get(Env[i, j], [1, 1, 1])\n",
    "\n",
    "    if annot_matrix is not False:\n",
    "        annot_matrix = np.where(Env == 1, '', annot_matrix)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(Env,fmt=\"\",  cmap=sns.color_palette([colors[i] for i in range(6)]), cbar=False,annot=annot_matrix, linewidths=0.5, linecolor='black')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_policy(Env, policy, title, curr_state = None) -> None:\n",
    "    \"\"\"\n",
    "    Function to plot the optimal policy matrix\n",
    "    \"\"\"\n",
    "    plot_env(Env, title, annot_matrix=False, show=False)\n",
    "    for i in range(20):\n",
    "        for j in range(20):\n",
    "            if policy[i, j] == \"up\":\n",
    "                plt.arrow(j+0.5, i+0.85, 0, -0.5, width=0.04, color='black')  # Up\n",
    "            if policy[i, j] == \"right\":\n",
    "                plt.arrow(j+0.15, i+0.5, 0.5, 0, width=0.04, color='black')  # Right\n",
    "            if policy[i, j] == \"down\":\n",
    "                plt.arrow(j+0.5, i+0.15, 0, 0.50, width=0.04, color='black')  # Down\n",
    "            if policy[i, j] == \"left\":\n",
    "                plt.arrow(j+0.85, i+0.5, -0.5, 0, width=0.04, color='black')  # Left\n",
    "\n",
    "    if curr_state:\n",
    "        circle = plt.Circle((curr_state[1] + 0.5, curr_state[0] + 0.5), 0.3, color='blue', fill=True, label='Current State')\n",
    "        plt.gca().add_artist(circle)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_optimal_path(Env, path, title) -> None:\n",
    "    \"\"\"\n",
    "    Function to plot the optimal path\n",
    "    \"\"\"\n",
    "    plot_env(Env, title, annot_matrix=False, show=False)\n",
    "    \n",
    "    for state_cr, direction in path:\n",
    "        r = state_cr[0] # x_coordinate\n",
    "        c = state_cr[1] # y_coordinate\n",
    "\n",
    "        if direction == 'right':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0.8, 0, width=0.04, color='black')   # Right\n",
    "        if direction == 'left':\n",
    "            plt.arrow(c + 0.5, r + 0.5, -0.8, 0, width=0.04, color='black')  # Left\n",
    "        if direction == 'up':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0, -0.8, width=0.04, color='black')  # Up\n",
    "        if direction == 'down':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0, 0.8, width=0.04, color='black')  # Down\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_reward(next_state_type, wall_hit) -> float:\n",
    "    \"\"\"\n",
    "    Function to return the reward of an action based on next state and if wall was hit\n",
    "    \"\"\"\n",
    "    next_state_type = int(next_state_type)\n",
    "    reward = -1 # For taking an action\n",
    "\n",
    "    if wall_hit:         # Wall\n",
    "        reward += -0.8\n",
    "\n",
    "    if next_state_type == 2:  # Oil\n",
    "        reward += -5\n",
    "    elif next_state_type == 3:  # Bump\n",
    "        reward += -10\n",
    "    elif next_state_type == 5:  # Goal\n",
    "        reward += 300\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def get_next_state(Env, state, action, p = None, deterministic = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Function to return the next state given the current state and action\n",
    "    \"\"\"\n",
    "    i, j = state[0], state[1]\n",
    "    wall_hit = False\n",
    "    \n",
    "    if not deterministic:\n",
    "        probabilities = get_probabilities(action, p)\n",
    "        action = np.random.choice(['up', 'down', 'left', 'right'], p=[probabilities['up'], probabilities['down'], probabilities['left'], probabilities['right']])\n",
    "\n",
    "    if action == 'up':\n",
    "        next_i, next_j = i - 1, j\n",
    "    elif action == 'down':\n",
    "        next_i, next_j = i + 1, j\n",
    "    elif action == 'left':\n",
    "        next_i, next_j = i, j - 1\n",
    "    elif action == 'right':\n",
    "        next_i, next_j = i, j + 1\n",
    "\n",
    "    if Env[next_i, next_j] == 1:  # Wall\n",
    "        next_i, next_j = i, j\n",
    "        wall_hit = True\n",
    "    \n",
    "    return next_i, next_j, wall_hit\n",
    "\n",
    "def get_probabilities(action, p) -> list:\n",
    "    \"\"\"\n",
    "    Function to return the probabilities of each action given the current action\n",
    "    \"\"\"\n",
    "    probabilities = {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 0}\n",
    "    if action == \"up\":\n",
    "        probabilities[\"up\"] = 1 - p\n",
    "        probabilities[\"down\"] = 0\n",
    "        probabilities[\"left\"] = p / 2\n",
    "        probabilities[\"right\"] = p / 2\n",
    "    elif action == \"down\":\n",
    "        probabilities[\"up\"] = 0\n",
    "        probabilities[\"down\"] = 1 - p\n",
    "        probabilities[\"left\"] = p / 2\n",
    "        probabilities[\"right\"] = p / 2\n",
    "    elif action == \"left\":\n",
    "        probabilities[\"up\"] = p / 2\n",
    "        probabilities[\"down\"] = p / 2\n",
    "        probabilities[\"left\"] = 1 - p\n",
    "        probabilities[\"right\"] = 0\n",
    "    elif action == \"right\":\n",
    "        probabilities[\"up\"] = p / 2\n",
    "        probabilities[\"down\"] = p / 2\n",
    "        probabilities[\"left\"] = 0\n",
    "        probabilities[\"right\"] = 1 - p\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def get_optimal_path(Env, start, end, optimal_policy):\n",
    "    \"\"\"\n",
    "    Function to get the optimal path from start to end\n",
    "    \"\"\"\n",
    "    curr_state = start\n",
    "    path = []\n",
    "    visited_states = []\n",
    "    optimal_path_found = True\n",
    "    while curr_state != end:\n",
    "        if curr_state in visited_states:\n",
    "            optimal_path_found = False\n",
    "            print(\"No optimal path found.\")\n",
    "            return path, optimal_path_found\n",
    "\n",
    "        visited_states.append(curr_state)\n",
    "        i, j = curr_state[0], curr_state[1]\n",
    "        action = optimal_policy[i, j]\n",
    "        path.append((curr_state, action))\n",
    "        next_i, next_j, wall_hit = get_next_state(Env, curr_state, action, deterministic = True)\n",
    "        curr_state = (next_i, next_j)\n",
    "\n",
    "    return path, optimal_path_found\n",
    "\n",
    "def get_random_state(Env):\n",
    "    \"\"\"\n",
    "    Function to get a random start state\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        i = np.random.randint(1, 19)\n",
    "        j = np.random.randint(1, 19)\n",
    "\n",
    "        if Env[i, j] != 1 and Env[i, j] != 5:  # should not be the goal state or a wall\n",
    "            break\n",
    "    return (i, j)\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Function to return the action based on epsilon greedy policy\n",
    "    \"\"\"\n",
    "    possible_actions = ['up', 'down', 'left', 'right']\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = np.random.choice(possible_actions)\n",
    "    else:\n",
    "        i, j = state[0], state[1]\n",
    "        action_index = np.argmax(Q[i, j])\n",
    "        action = possible_actions[action_index]\n",
    "\n",
    "    return action\n",
    "\n",
    "def get_preferences(H, state):\n",
    "    \"\"\"\n",
    "    Function to return the preferences of each action given H and a state.\n",
    "    \"\"\"\n",
    "    logits = H[state[0], state[1]]\n",
    "    max_logit = np.max(logits)  # for stability\n",
    "    exp_logits = np.exp(logits - max_logit)\n",
    "    preferences = exp_logits / np.sum(exp_logits)\n",
    "    return preferences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for TD algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(Env, p, gamma, alpha, epsilon, num_episodes, max_episode_length):\n",
    "    \"\"\"\n",
    "    Function to perform Q-Learning\n",
    "    \"\"\"\n",
    "    Q = np.zeros((Env.shape[0], Env.shape[1], 4))\n",
    "    policy = np.full((Env.shape[0], Env.shape[1]), '', dtype=object)\n",
    "    possible_actions = ['up', 'down', 'left', 'right']\n",
    "    acc_rewards = [0]\n",
    "\n",
    "    # Repeating for each episode\n",
    "    for _ in range(num_episodes):\n",
    "        curr_state = get_random_state(Env)\n",
    "        episode_length = 0\n",
    "        ep_acc_reward = 0\n",
    "\n",
    "        # Repeating for each step of the episode\n",
    "        while episode_length < max_episode_length:\n",
    "            i, j = curr_state[0], curr_state[1]\n",
    "            action = epsilon_greedy(Q, curr_state, epsilon)\n",
    "\n",
    "            next_i, next_j, wall_hit = get_next_state(Env, curr_state, action, p)\n",
    "            reward = get_reward(Env[next_i, next_j], wall_hit)\n",
    "            ep_acc_reward += reward\n",
    "            next_state = (next_i, next_j)\n",
    "            curr_Q = Q[i, j, possible_actions.index(action)]\n",
    "\n",
    "            Q[i, j, possible_actions.index(action)] = curr_Q + alpha * (reward + gamma * np.max(Q[next_i, next_j]) - curr_Q)\n",
    "            policy[i, j] = possible_actions[np.argmax(Q[i, j])]\n",
    "\n",
    "            curr_state = next_state\n",
    "            episode_length += 1\n",
    "        \n",
    "        avg_ep_acc_reward = ep_acc_reward / episode_length\n",
    "        acc_rewards.append(avg_ep_acc_reward)\n",
    "\n",
    "    return policy, Q, acc_rewards\n",
    "\n",
    "def sarsa(Env, p, gamma, alpha, epsilon, num_episodes, max_episode_length):\n",
    "    \"\"\"\n",
    "    Function to perform SARSA\n",
    "    \"\"\"\n",
    "    Q = np.zeros((Env.shape[0], Env.shape[1], 4))\n",
    "    policy = np.full((Env.shape[0], Env.shape[1]), '', dtype=object)\n",
    "    possible_actions = ['up', 'down', 'left', 'right']\n",
    "    acc_rewards = [0]\n",
    "\n",
    "    # Repeating for each episode\n",
    "    for _ in range(num_episodes):\n",
    "        curr_state = get_random_state(Env)\n",
    "        curr_action = epsilon_greedy(Q, curr_state, epsilon)\n",
    "        episode_length = 0\n",
    "        ep_acc_reward = 0\n",
    "\n",
    "        # Repeating for each step of the episode\n",
    "        while episode_length < max_episode_length:\n",
    "            i, j = curr_state[0], curr_state[1]\n",
    "\n",
    "            next_i, next_j, wall_hit = get_next_state(Env, curr_state, curr_action, p)\n",
    "            reward = get_reward(Env[next_i, next_j], wall_hit)\n",
    "\n",
    "            curr_Q = Q[i, j, possible_actions.index(curr_action)]\n",
    "            ep_acc_reward += reward\n",
    "\n",
    "            next_state = (next_i, next_j)\n",
    "            next_action = epsilon_greedy(Q, next_state, epsilon)\n",
    "            next_Q = Q[next_i, next_j, possible_actions.index(next_action)]\n",
    "\n",
    "            Q[i, j, possible_actions.index(curr_action)] = curr_Q + alpha * (reward + gamma * next_Q - curr_Q)\n",
    "            policy[i, j] = possible_actions[np.argmax(Q[i, j])]\n",
    "\n",
    "            curr_state = next_state\n",
    "            curr_action = next_action\n",
    "            episode_length += 1\n",
    "        \n",
    "        avg_ep_acc_reward = ep_acc_reward / episode_length\n",
    "        acc_rewards.append(avg_ep_acc_reward)\n",
    "\n",
    "    return policy, Q, acc_rewards\n",
    "\n",
    "def actor_critic(Env, p, gamma, alpha, beta, num_episodes, max_episode_length):\n",
    "    \"\"\"\n",
    "    Refactored Actor-Critic algorithm with stable softmax, full gradient updates, and better initialization.\n",
    "    \"\"\"\n",
    "    rows, cols = Env.shape\n",
    "    V = np.random.normal(0, 0.01, (rows, cols))\n",
    "    H = np.random.normal(0, 0.01, (rows, cols, 4))\n",
    "    policy = np.full((rows, cols), '', dtype=object)\n",
    "    possible_actions = ['up', 'down', 'left', 'right']\n",
    "    acc_rewards = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        curr_state = get_random_state(Env)\n",
    "        episode_length = 0\n",
    "        ep_acc_reward = 0\n",
    "\n",
    "        while episode_length < max_episode_length:\n",
    "            i, j = curr_state\n",
    "            preferences = get_preferences(H, curr_state)\n",
    "\n",
    "            curr_action_idx = np.random.choice(range(4), p=preferences)\n",
    "            curr_action = possible_actions[curr_action_idx]\n",
    "\n",
    "            next_i, next_j, wall_hit = get_next_state(Env, curr_state, curr_action, p)\n",
    "            reward = get_reward(Env[next_i, next_j], wall_hit)\n",
    "            ep_acc_reward += reward\n",
    "            next_state = (next_i, next_j)\n",
    "\n",
    "            delta = reward + gamma * V[next_i, next_j] - V[i, j]\n",
    "            V[i, j] += alpha * delta\n",
    "            H[i, j, curr_action_idx] += beta * delta * (1 - preferences[curr_action_idx])\n",
    "\n",
    "            policy[i, j] = possible_actions[np.argmax(H[i, j])]\n",
    "\n",
    "            curr_state = next_state\n",
    "            episode_length += 1\n",
    "\n",
    "        avg_ep_reward = ep_acc_reward / episode_length if episode_length > 0 else 0\n",
    "        acc_rewards.append(avg_ep_reward)\n",
    "\n",
    "    return policy, H, acc_rewards\n",
    "\n",
    "def simulate(algo_name, num_runs, p, gamma, alpha = 0.01, beta = 0.05, epsilon = 0.1, num_episodes = 5000, max_episode_length = 200):\n",
    "    \"\"\"\n",
    "    Function to simulate TD algorithms for multiple runs\n",
    "    \"\"\"\n",
    "    optimal_path_found_counts = 0\n",
    "    run_acc_rewards = []\n",
    "    for i in range(num_runs):\n",
    "        Env = init_env()\n",
    "        print(f\"\\nRun {i+1}\")\n",
    "\n",
    "        if algo_name == \"Q-Learning\":\n",
    "            policy, Q, acc_rewards = q_learning(Env, p, gamma, alpha, epsilon, num_episodes, max_episode_length)\n",
    "\n",
    "        elif algo_name == \"SARSA\":\n",
    "            policy, Q, acc_rewards = sarsa(Env, p, gamma, alpha, epsilon, num_episodes, max_episode_length)\n",
    "        \n",
    "        elif algo_name == \"Actor_Critic\":\n",
    "            policy, H, acc_rewards = actor_critic(Env, p, gamma, alpha, beta, num_episodes, max_episode_length)\n",
    "\n",
    "        run_acc_rewards.append(acc_rewards)\n",
    "        \n",
    "        plot_policy(Env, policy, f\"{algo_name} Optimal Policy - Run {i+1}\")\n",
    "\n",
    "        optimal_path, optimal_path_found = get_optimal_path(Env, START_STATE, GOAL_STATE, policy)\n",
    "        plot_optimal_path(Env, optimal_path, f\"{algo_name} Optimal Path - Run {i+1}\")\n",
    "\n",
    "        if optimal_path_found:\n",
    "            optimal_path_found_counts += 1\n",
    "\n",
    "    print(f\"Optiaml Path Found: {optimal_path_found_counts}\\n\")\n",
    "\n",
    "    avg_acc_rewards = calculate_avg_acc_rewards(run_acc_rewards, num_episodes, num_runs)\n",
    "    return avg_acc_rewards\n",
    "\n",
    "def simulate_all(num_runs, p, gamma, alpha, beta, epsilon, num_episodes, max_episode_length):\n",
    "    algos = [\"Actor_Critic\", \"Q-Learning\", \"SARSA\"]\n",
    "    avg_acc_rewards = {algo: None for algo in algos}\n",
    "    for algo in algos:\n",
    "        print(algo)\n",
    "        if algo == \"Actor_Critic\":\n",
    "            avg_acc_rewards[algo] = simulate(algo, num_runs, p, gamma)\n",
    "        else:\n",
    "            avg_acc_rewards[algo] = simulate(algo, num_runs, p, gamma, alpha, beta, epsilon, num_episodes, max_episode_length)\n",
    "    \n",
    "    plot_avg_acc_rewards(avg_acc_rewards, f\"Average Accumulated Rewards\")\n",
    "    \n",
    "    return avg_acc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.025\n",
    "gamma = 0.96\n",
    "alpha = 0.25\n",
    "beta = 0.05\n",
    "epsilon = 0.1\n",
    "max_episode_length = 1000\n",
    "num_episodes = 1000\n",
    "num_runs = 10\n",
    "\n",
    "avg_acc_rewards = simulate_all(num_runs, p, gamma, alpha, beta, epsilon, num_episodes, max_episode_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc_rewards_new = {\n",
    "    \"Q-Learning\":avg_acc_rewards[\"Q-Learning\"],\n",
    "    \"SARSA\": avg_acc_rewards[\"SARSA\"]\n",
    "}\n",
    "plot_avg_acc_rewards(avg_acc_rewards_new, f\"Average Accumulated Rewards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc_rewards_new = {\n",
    "    \"Actor_Critic\": avg_acc_rewards[\"Actor_Critic\"],\n",
    "}\n",
    "plot_avg_acc_rewards(avg_acc_rewards_new, f\"Average Accumulated Rewards\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "NUM_STATES = 16\n",
    "NUM_ACTIONS = 4\n",
    "NUM_GENES = 4\n",
    "ACTIONS = {'a1': [0, 0, 0, 0], 'a2': [0, 1, 0, 0], 'a3': [0, 0, 1, 0], 'a4': [0, 0, 0, 1]}\n",
    "STATES = {'s1': [0, 0, 0, 0], 's2': [0, 0, 0, 1], 's3': [0, 0, 1, 0], 's4': [0, 0, 1, 1], 's5': [0, 1, 0, 0], 's6': [0, 1, 0, 1], 's7': [0, 1, 1, 0], 's8': [0, 1, 1, 1], 's9': [1, 0, 0, 0], 's10': [1, 0, 0, 1], 's11': [1, 0, 1, 0], 's12': [1, 0, 1, 1], 's13': [1, 1, 0, 0], 's14': [1, 1, 0, 1], 's15': [1, 1, 1, 0], 's16': [1, 1, 1, 1]}\n",
    "C = np.array([[0, 0, -1, 0],\n",
    "              [1, 0, -1, -1],\n",
    "              [0, 1, 0, 0],\n",
    "              [-1, 1, 1, 0]])\n",
    "\n",
    "\n",
    "def xor_mod2(vec1, vec2):\n",
    "    \"\"\"Performs component-wise XOR (mod 2 addition).\"\"\"\n",
    "    return (vec1 + vec2) % 2\n",
    "\n",
    "def v_bar(vec):\n",
    "    output = np.zeros(len(vec))\n",
    "    for i, elem in enumerate(vec):\n",
    "        if elem > 0:\n",
    "            output[i] = 1\n",
    "        else:\n",
    "            output[i] = 0\n",
    "    \n",
    "    return output\n",
    "\n",
    "def compute_N(p):\n",
    "    \"\"\"Returns a list of length NUM_GENES, where each element is 1 with probability p and 0 with probability 1-p.\"\"\"\n",
    "    N = []\n",
    "    for _ in range(NUM_GENES):\n",
    "        N.append(int(np.random.rand() < p))\n",
    "    return N\n",
    "\n",
    "def get_random_state_idx():\n",
    "    \"\"\"Returns a random state index.\"\"\"\n",
    "    random_idx = np.random.randint(0, NUM_STATES)\n",
    "    return random_idx\n",
    "\n",
    "def compute_next_state_idx(p, state_idx, action_idx):\n",
    "    \"\"\"Computes the next state index for a given action index and state index.\"\"\"\n",
    "    N = compute_N(p)\n",
    "    action = ACTIONS[f'a{action_idx+1}']\n",
    "    state = STATES[f\"s{state_idx+1}\"]\n",
    "    next_state = list(xor_mod2(xor_mod2(v_bar(C @ state), action), N))\n",
    "    next_state_idx = list(STATES.values()).index(next_state)\n",
    "\n",
    "    return next_state_idx\n",
    "\n",
    "def action_cost(action_idx):\n",
    "    \"\"\"Computes the cost of an action.\"\"\"\n",
    "    if action_idx == \"a1\" or action_idx == \"a4\":\n",
    "        return 0\n",
    "    \n",
    "    return 1\n",
    "\n",
    "def init_reward_function():\n",
    "    \"\"\"Defines the reward function.\"\"\"\n",
    "    R = {}\n",
    "    for action in list(ACTIONS.keys()):\n",
    "        R[action] = np.zeros((NUM_STATES, NUM_STATES))\n",
    "\n",
    "        for i in range(NUM_STATES):\n",
    "            for j in range(NUM_STATES):\n",
    "                next_state = STATES[f\"s{j+1}\"]\n",
    "                R[action][i, j] = 5 * sum(next_state) - action_cost(action)\n",
    "\n",
    "    return R\n",
    "\n",
    "def epsilon_greedy(Q, state_idx, epsilon):\n",
    "    \"\"\"\n",
    "    Function to return the action based on epsilon greedy policy\n",
    "    \"\"\"\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action_index = np.random.randint(0, NUM_ACTIONS)\n",
    "    else:\n",
    "        action_index = np.argmax(Q[state_idx])\n",
    "\n",
    "    return action_index\n",
    "\n",
    "def get_preference(H, curr_state_idx, action_idx = None):\n",
    "    \"\"\"\n",
    "    Function to return the preferences of each action given H and a state.\n",
    "    \"\"\"\n",
    "    sigma_e = 0\n",
    "\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        sigma_e += power(np.e, H[curr_state_idx, i])\n",
    "\n",
    "    if action_idx is None:\n",
    "        preferences = np.zeros(NUM_ACTIONS)\n",
    "        for i in range(NUM_ACTIONS):\n",
    "            preferences[i] = power(np.e, H[curr_state_idx, i]) / sigma_e\n",
    "\n",
    "        return preferences\n",
    "\n",
    "    preference = power(np.e, H[curr_state_idx, action_idx]) / sigma_e\n",
    "    return preference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(p, gamma, alpha, epsilon, num_episodes, max_episode_length):\n",
    "    \"\"\"\n",
    "    Function to perform Q-Learning\n",
    "    \"\"\"\n",
    "    Q = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
    "    R = init_reward_function()\n",
    "    policy = np.full(NUM_STATES, '', dtype=object)\n",
    "    acc_rewards = [0]\n",
    "\n",
    "    # Repeating for each episode\n",
    "    for _ in range(num_episodes):\n",
    "        curr_state_idx = get_random_state_idx()\n",
    "        episode_length = 0\n",
    "        ep_acc_reward = 0\n",
    "\n",
    "        # Repeating for each step of the episode\n",
    "        while episode_length < max_episode_length:\n",
    "            action_idx = epsilon_greedy(Q, curr_state_idx, epsilon)\n",
    "            action = f\"a{action_idx+1}\"\n",
    "            next_state_idx = compute_next_state_idx(p, curr_state_idx, action_idx)\n",
    "            reward = R[action][curr_state_idx, next_state_idx]\n",
    "            ep_acc_reward += reward\n",
    "            curr_Q = Q[curr_state_idx, action_idx]\n",
    "\n",
    "            Q[curr_state_idx, action_idx] = curr_Q + alpha * (reward + gamma * np.max(Q[next_state_idx]) - curr_Q)\n",
    "            policy[curr_state_idx] = f\"a{np.argmax(Q[curr_state_idx])+1}\"\n",
    "\n",
    "            curr_state_idx = next_state_idx\n",
    "            episode_length += 1\n",
    "        \n",
    "        avg_ep_acc_reward = ep_acc_reward / episode_length\n",
    "        acc_rewards.append(avg_ep_acc_reward)\n",
    "\n",
    "    return policy, Q, acc_rewards\n",
    "\n",
    "def sarsa(p, gamma, alpha, epsilon, num_episodes, max_episode_length):\n",
    "    \"\"\"\n",
    "    Function to perform SARSA\n",
    "    \"\"\"\n",
    "    Q = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
    "    R = init_reward_function()\n",
    "    policy = np.full(NUM_STATES, '', dtype=object)\n",
    "    acc_rewards = [0]\n",
    "\n",
    "    # Repeating for each episode\n",
    "    for _ in range(num_episodes):\n",
    "        curr_state_idx = get_random_state_idx()\n",
    "        curr_action_idx = epsilon_greedy(Q, curr_state_idx, epsilon)\n",
    "        episode_length = 0\n",
    "        ep_acc_reward = 0\n",
    "\n",
    "        # Repeating for each step of the episode\n",
    "        while episode_length < max_episode_length:\n",
    "            next_state_idx = compute_next_state_idx(p, curr_state_idx, curr_action_idx)\n",
    "            curr_action = f\"a{curr_action_idx+1}\"\n",
    "            reward = R[curr_action][curr_state_idx, next_state_idx]\n",
    "\n",
    "            curr_Q = Q[curr_state_idx, curr_action_idx]\n",
    "            ep_acc_reward += reward\n",
    "\n",
    "            next_action_idx = epsilon_greedy(Q, next_state_idx, epsilon)\n",
    "            next_Q = Q[next_state_idx, next_action_idx]\n",
    "\n",
    "            Q[curr_state_idx, curr_action_idx] = curr_Q + alpha * (reward + gamma * next_Q - curr_Q)\n",
    "            policy[curr_state_idx] = f\"a{np.argmax(Q[curr_state_idx])+1}\"\n",
    "\n",
    "            curr_state_idx = next_state_idx\n",
    "            curr_action_idx = next_action_idx\n",
    "            episode_length += 1\n",
    "        \n",
    "        avg_ep_acc_reward = ep_acc_reward / episode_length\n",
    "        acc_rewards.append(avg_ep_acc_reward)\n",
    "\n",
    "    return policy, Q, acc_rewards\n",
    "\n",
    "def actor_critic(p, gamma, alpha, beta, num_episodes, max_episode_length):\n",
    "    \"\"\"\n",
    "    Function to perform Actor-Critic method\n",
    "    \"\"\"\n",
    "    V = np.zeros((NUM_STATES))\n",
    "    H = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
    "    R = init_reward_function()\n",
    "    policy = np.full(NUM_STATES, '', dtype=object)\n",
    "    acc_rewards = [0]\n",
    "\n",
    "    # Repeating for each episode\n",
    "    for _ in range(num_episodes):\n",
    "        curr_state_idx = get_random_state_idx()\n",
    "        episode_length = 0\n",
    "        ep_acc_reward = 0\n",
    "\n",
    "        # Repeating for each step of the episode\n",
    "        while episode_length < max_episode_length:\n",
    "            preferences = get_preference(H, curr_state_idx)\n",
    "            curr_action_idx = np.random.choice(range(NUM_ACTIONS), p=preferences)\n",
    "            curr_action = f\"a{curr_action_idx+1}\"\n",
    "            next_state_idx = compute_next_state_idx(p, curr_state_idx, curr_action_idx)\n",
    "            reward = R[curr_action][curr_state_idx, next_state_idx]\n",
    "            ep_acc_reward += reward\n",
    "\n",
    "            curr_H = H[curr_state_idx, curr_action_idx]\n",
    "            curr_action_preference = get_preference(H, curr_state_idx, curr_action_idx)\n",
    "\n",
    "            delta = reward + gamma * V[next_state_idx] - V[curr_state_idx]\n",
    "            V[curr_state_idx] = V[curr_state_idx] + alpha * delta\n",
    "            H[curr_state_idx, curr_action_idx] = curr_H + beta * delta * (1 - curr_action_preference)\n",
    "            policy[curr_state_idx] = f\"a{np.argmax(H[curr_state_idx])+1}\"\n",
    "\n",
    "            curr_state_idx = next_state_idx\n",
    "            episode_length += 1\n",
    "        \n",
    "        avg_ep_acc_reward = ep_acc_reward / episode_length\n",
    "        acc_rewards.append(avg_ep_acc_reward)\n",
    "\n",
    "    return policy, H, acc_rewards\n",
    "\n",
    "def sarsa_lambda(p, gamma, alpha, epsilon, num_episodes, max_episode_length, lambda_val):\n",
    "    \"\"\"\n",
    "    Function to perform SARSA(lambda)\n",
    "    \"\"\"\n",
    "    Q = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
    "    R = init_reward_function()\n",
    "    policy = np.full(NUM_STATES, '', dtype=object)\n",
    "    acc_rewards = [0]\n",
    "\n",
    "    # Repeating for each episode\n",
    "    for _ in range(num_episodes):\n",
    "        E = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
    "        curr_state_idx = get_random_state_idx()\n",
    "        curr_action_idx = epsilon_greedy(Q, curr_state_idx, epsilon)\n",
    "        episode_length = 0\n",
    "        ep_acc_reward = 0\n",
    "\n",
    "        # Repeating for each step of the episode\n",
    "        while episode_length < max_episode_length:\n",
    "            next_state_idx = compute_next_state_idx(p, curr_state_idx, curr_action_idx)\n",
    "            curr_action = f\"a{curr_action_idx+1}\"\n",
    "            reward = R[curr_action][curr_state_idx, next_state_idx]\n",
    "            ep_acc_reward += reward\n",
    "            next_action_idx = epsilon_greedy(Q, next_state_idx, epsilon)\n",
    "\n",
    "            delta = reward + gamma * Q[next_state_idx, next_action_idx] - Q[curr_state_idx, curr_action_idx]\n",
    "            E[curr_state_idx, curr_action_idx] += 1\n",
    "            Q = Q + alpha * delta * E\n",
    "            E = gamma * lambda_val * E\n",
    "            policy[curr_state_idx] = f\"a{np.argmax(Q[curr_state_idx])+1}\"\n",
    "\n",
    "            curr_state_idx = next_state_idx\n",
    "            curr_action_idx = next_action_idx\n",
    "            episode_length += 1\n",
    "        \n",
    "        avg_ep_acc_reward = ep_acc_reward / episode_length\n",
    "        acc_rewards.append(avg_ep_acc_reward)\n",
    "\n",
    "    return policy, Q, acc_rewards\n",
    "\n",
    "def simulate(algo_name, num_runs, p, gamma, alpha, beta, epsilon, lambda_val, num_episodes, max_episode_length):\n",
    "    \"\"\"\n",
    "    Function to simulate TD algorithms for multiple runs\n",
    "    \"\"\"\n",
    "    run_acc_rewards = []\n",
    "    for i in range(num_runs):\n",
    "\n",
    "        if algo_name == \"Q-Learning\":\n",
    "            policy, Q, acc_rewards = q_learning(p, gamma, alpha, epsilon, num_episodes, max_episode_length)\n",
    "\n",
    "        elif algo_name == \"SARSA\":\n",
    "            policy, Q, acc_rewards = sarsa(p, gamma, alpha, epsilon, num_episodes, max_episode_length)\n",
    "        \n",
    "        elif algo_name == \"Actor_Critic\":\n",
    "            policy, H, acc_rewards = actor_critic(p, gamma, alpha, beta, num_episodes, max_episode_length)\n",
    "\n",
    "        elif algo_name == \"sarsa_lambda\":\n",
    "            policy, H, acc_rewards = sarsa_lambda(p, gamma, alpha, epsilon, num_episodes, max_episode_length, lambda_val)\n",
    "\n",
    "        run_acc_rewards.append(acc_rewards)\n",
    "        \n",
    "        print(policy)\n",
    "        print(f\"Run {i+1} completed.\")\n",
    "\n",
    "    avg_acc_rewards = calculate_avg_acc_rewards(run_acc_rewards, num_episodes, num_runs)\n",
    "    \n",
    "    return avg_acc_rewards\n",
    "\n",
    "def simulate_all(num_runs, p, gamma, alpha, beta, epsilon, lambda_val, num_episodes, max_episode_length):\n",
    "    algos = [\"Q-Learning\", \"SARSA\", \"Actor_Critic\", \"sarsa_lambda\"]\n",
    "    avg_acc_rewards = {algo: None for algo in algos}\n",
    "\n",
    "    for algo in algos:\n",
    "        print(algo)\n",
    "        avg_acc_rewards[algo] = simulate(algo, num_runs, p, gamma, alpha, beta, epsilon, lambda_val, num_episodes, max_episode_length)\n",
    "    \n",
    "    plot_avg_acc_rewards(avg_acc_rewards, f\"Average Accumulated Rewards\")\n",
    "    \n",
    "    return avg_acc_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.1\n",
    "gamma = 0.9\n",
    "alpha = 0.25\n",
    "beta = 0.05\n",
    "lambda_val = 0.95\n",
    "epsilon = 0.15\n",
    "max_episode_length = 100\n",
    "num_episodes = 1000\n",
    "num_runs = 10\n",
    "\n",
    "avg_acc_rewards = simulate_all(num_runs, p, gamma, alpha, beta, epsilon, lambda_val, num_episodes, max_episode_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
