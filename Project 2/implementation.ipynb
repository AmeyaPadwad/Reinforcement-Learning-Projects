{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_STATE = (3, 13)    \n",
    "\n",
    "def init_env() -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to initialize the environment matrix\n",
    "    \"\"\"\n",
    "    # Environment matrix\n",
    "    # regular = 0    wall = 1    oil = 2     bump = 3    start = 4    end = 5\n",
    "    Env = np.zeros([20, 20])\n",
    "\n",
    "    # Defining border walls\n",
    "    Env[:, [0]] = np.ones([20, 1])\n",
    "    Env[:, [19]] = np.ones([20, 1])\n",
    "    Env[0, :] = np.ones([1, 20])\n",
    "    Env[19, :] = np.ones([1, 20])\n",
    "\n",
    "    # Defining cell properties inside the maze\n",
    "    # wall = 1\n",
    "    Env[2, 5] = 1\n",
    "    Env[3, 5] = 1\n",
    "    Env[4, 3:17] = np.ones([1, 14])\n",
    "    Env[5, 3] = 1\n",
    "    Env[6, 3] = 1; Env[6, 6] = 1; Env[6, 9] = 1; Env[6, 15] = 1\n",
    "    Env[7, 3] = 1; Env[7, 6] = 1; Env[7, 9] = 1; Env[7, 12:16] = np.ones([1, 4])\n",
    "    Env[8, 6] = 1; Env[8, 9] = 1; Env[8, 15] = 1\n",
    "    Env[9, 6] = 1; Env[9, 9] = 1; Env[9, 15] = 1\n",
    "    Env[10, 1:5] = np.ones([1, 4]); Env[10, 6] = 1; Env[10, 9:11] = [1, 1]; Env[10, 15] = 1\n",
    "    Env[11, 6] = 1; Env[11, 10] = 1; Env[11, 13] = 1; Env[11, 15:18] = np.ones([1, 3])\n",
    "    Env[12, 3:8] = np.ones([1, 5]); Env[12, 10] = 1; Env[12, 13] = 1; Env[12, 17] = 1\n",
    "    Env[13, 7] = 1; Env[13, 10] = 1; Env[13, 13] = 1; Env[13, 17] = 1\n",
    "    Env[14, 7] = 1; Env[14, 10] = 1; Env[14, 13] = 1\n",
    "    Env[15, 7] = 1; Env[15, 13:17] = [1, 1, 1, 1]\n",
    "    Env[17, 1:3] = [1, 1]; Env[17, 7:13] = [1, 1, 1, 1, 1, 1]\n",
    "\n",
    "    # oil = 2\n",
    "    Env[2, 8] = 2; Env[2, 16] = 2\n",
    "    Env[4, 2] = 2\n",
    "    Env[5, 6] = 2\n",
    "    Env[10, 18] = 2\n",
    "    Env[14, 14] = 2\n",
    "    Env[15, 10] = 2\n",
    "    Env[16, 10] = 2\n",
    "    Env[17, 14] = 2; Env[17, 17] = 2\n",
    "    Env[18, 7] = 2\n",
    "\n",
    "    # bump = 3\n",
    "    Env[1, 11] = 3; Env[1, 12] = 3\n",
    "    Env[2, 1:4] = [3, 3, 3]\n",
    "    Env[5, 1] = 3; Env[5, 9] = 3; Env[5, 17] = 3\n",
    "    Env[6, 17] = 3\n",
    "    Env[7, 2] = 3; Env[7, 10:12] = [3, 3]; Env[7, 17] = 3\n",
    "    Env[8, 17] = 3\n",
    "    Env[12, 11:13] = [3, 3]\n",
    "    Env[14, 1:3] = [3, 3]\n",
    "    Env[15, 17:19] = [3, 3]\n",
    "    Env[16, 7] = 3\n",
    "\n",
    "    # start = 4\n",
    "    Env[15, 4] = 4\n",
    "\n",
    "    # goal = 5\n",
    "    Env[GOAL_STATE[0], GOAL_STATE[1]] = 5\n",
    "\n",
    "    return Env\n",
    "\n",
    "def plot_env(Env, title, annot_matrix=False, show=True) -> None:\n",
    "    \"\"\"\n",
    "    Function to plot the environment matrix\n",
    "    \"\"\"\n",
    "    # Define colors\n",
    "    colors = {\n",
    "        0: [1, 1, 1],        # White\n",
    "        1: [0, 0, 0],        # Black\n",
    "        2: [0.55, 0, 0],     # Light Brown\n",
    "        3: [0.96, 0.8, 0.6], # Dark Red\n",
    "        4: [0, 0, 1],        # Green\n",
    "        5: [0, 1, 0]         # Blue\n",
    "    }\n",
    "\n",
    "\n",
    "    rgb_maze = np.zeros((Env.shape[0], Env.shape[1], 3))\n",
    "    for i in range(Env.shape[0]):\n",
    "        for j in range(Env.shape[1]):\n",
    "            rgb_maze[i, j] = colors.get(Env[i, j], [1, 1, 1])\n",
    "\n",
    "    if annot_matrix is not False:\n",
    "        annot_matrix = np.where(Env == 1, '', annot_matrix)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(Env,fmt=\"\",  cmap=sns.color_palette([colors[i] for i in range(6)]), cbar=False,annot=annot_matrix, linewidths=0.5, linecolor='black')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_policy(Env, policy, title) -> None:\n",
    "    \"\"\"\n",
    "    Function to plot the optimal policy matrix\n",
    "    \"\"\"\n",
    "    plot_env(Env, title, annot_matrix=False, show=False)\n",
    "    for i in range(20):\n",
    "        for j in range(20):\n",
    "            if policy[i, j] == \"up\":\n",
    "                plt.arrow(j+0.5, i+0.85, 0, -0.5, width=0.04, color='black')  # Up\n",
    "            if policy[i, j] == \"right\":\n",
    "                plt.arrow(j+0.15, i+0.5, 0.5, 0, width=0.04, color='black')  # Right\n",
    "            if policy[i, j] == \"down\":\n",
    "                plt.arrow(j+0.5, i+0.15, 0, 0.50, width=0.04, color='black')  # Down\n",
    "            if policy[i, j] == \"left\":\n",
    "                plt.arrow(j+0.85, i+0.5, -0.5, 0, width=0.04, color='black')  # Left\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_optimal_path(Env, path, title) -> None:\n",
    "    \"\"\"\n",
    "    Function to plot the optimal path\n",
    "    \"\"\"\n",
    "    plot_env(Env, title, annot_matrix=False, show=False)\n",
    "    \n",
    "    for state_cr, direction in path:\n",
    "        r = state_cr[0] # x_coordinate\n",
    "        c = state_cr[1] # y_coordinate\n",
    "\n",
    "        if direction == 'right':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0.8, 0, width=0.04, color='black')   # Right\n",
    "        if direction == 'left':\n",
    "            plt.arrow(c + 0.5, r + 0.5, -0.8, 0, width=0.04, color='black')  # Left\n",
    "        if direction == 'up':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0, -0.8, width=0.04, color='black')  # Up\n",
    "        if direction == 'down':\n",
    "            plt.arrow(c + 0.5, r + 0.5, 0, 0.8, width=0.04, color='black')  # Down\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_reward(next_state_type, wall_hit) -> float:\n",
    "    \"\"\"\n",
    "    Function to return the reward of an action based on next state and if wall was hit\n",
    "    \"\"\"\n",
    "    next_state_type = int(next_state_type)\n",
    "    reward = -1 # For taking an action\n",
    "\n",
    "    if wall_hit:         # Wall\n",
    "        reward += -0.8\n",
    "\n",
    "    if next_state_type == 2:  # Oil\n",
    "        reward += -5\n",
    "    elif next_state_type == 3:  # Bump\n",
    "        reward += -10\n",
    "    elif next_state_type == 5:  # Goal\n",
    "        reward += 300\n",
    "    \n",
    "    return reward\n",
    "\n",
    "def get_next_state(Env, state, action) -> tuple:\n",
    "    \"\"\"\n",
    "    Function to return the next state given the current state and action\n",
    "    \"\"\"\n",
    "    i, j = state[0], state[1]\n",
    "    wall_hit = False\n",
    "\n",
    "    if action == 'up':\n",
    "        next_i, next_j = i - 1, j\n",
    "    elif action == 'down':\n",
    "        next_i, next_j = i + 1, j\n",
    "    elif action == 'left':\n",
    "        next_i, next_j = i, j - 1\n",
    "    elif action == 'right':\n",
    "        next_i, next_j = i, j + 1\n",
    "\n",
    "    if Env[next_i, next_j] == 1:  # Wall\n",
    "        next_i, next_j = i, j\n",
    "        wall_hit = True\n",
    "    \n",
    "    return next_i, next_j, wall_hit\n",
    "\n",
    "def get_probabilities(action, p) -> list:\n",
    "    \"\"\"\n",
    "    Function to return the probabilities of each action given the current action\n",
    "    \"\"\"\n",
    "    probabilities = {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 0}\n",
    "    if action == \"up\":\n",
    "        probabilities[\"up\"] = 1 - p\n",
    "        probabilities[\"down\"] = 0\n",
    "        probabilities[\"left\"] = p / 2\n",
    "        probabilities[\"right\"] = p / 2\n",
    "    elif action == \"down\":\n",
    "        probabilities[\"up\"] = 0\n",
    "        probabilities[\"down\"] = 1 - p\n",
    "        probabilities[\"left\"] = p / 2\n",
    "        probabilities[\"right\"] = p / 2\n",
    "    elif action == \"left\":\n",
    "        probabilities[\"up\"] = p / 2\n",
    "        probabilities[\"down\"] = p / 2\n",
    "        probabilities[\"left\"] = 1 - p\n",
    "        probabilities[\"right\"] = 0\n",
    "    elif action == \"right\":\n",
    "        probabilities[\"up\"] = p / 2\n",
    "        probabilities[\"down\"] = p / 2\n",
    "        probabilities[\"left\"] = 0\n",
    "        probabilities[\"right\"] = 1 - p\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "def get_optimal_path(Env, start, end, optimal_policy):\n",
    "    \"\"\"\n",
    "    Function to get the optimal path from start to end\n",
    "    \"\"\"\n",
    "    curr_state = start\n",
    "    path = []\n",
    "    visited_states = []\n",
    "    while curr_state != end:\n",
    "        if curr_state in visited_states:\n",
    "            print(\"No optimal path found.\")\n",
    "            return path\n",
    "\n",
    "        visited_states.append(curr_state)\n",
    "        i, j = curr_state[0], curr_state[1]\n",
    "        action = optimal_policy[i, j]\n",
    "        path.append((curr_state, action))\n",
    "        next_i, next_j, wall_hit = get_next_state(Env, curr_state, action)\n",
    "        curr_state = (next_i, next_j)\n",
    "\n",
    "    return path\n",
    "\n",
    "def calculate_state_value(Env, curr_state, action, p, V, gamma):\n",
    "    \"\"\"\n",
    "    Function to calculate the value of a state given the next state and reward\n",
    "    \"\"\"\n",
    "    i, j = curr_state[0], curr_state[1]\n",
    "    probabilites = get_probabilities(action, p)\n",
    "\n",
    "    possible_actions = ['up', 'down', 'left', 'right']\n",
    "    state_value = 0\n",
    "\n",
    "    for curr_action in possible_actions:\n",
    "        curr_action_prob = probabilites[curr_action]\n",
    "        \n",
    "        next_i, next_j, wall_hit = get_next_state(Env, (i, j), curr_action)\n",
    "        reward = get_reward(Env[next_i, next_j], wall_hit)\n",
    "\n",
    "        state_value += curr_action_prob * (reward + (gamma * V[next_i, next_j]))\n",
    "\n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(Env, policy, p, gamma, theta):\n",
    "    \"\"\"\n",
    "    Function to evaluate the policy using approximate policy evaluation\n",
    "    \"\"\"\n",
    "    V = np.zeros(Env.shape)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        V_new = np.copy(V)\n",
    "\n",
    "        for i in range(Env.shape[0]):\n",
    "            for j in range(Env.shape[1]):\n",
    "                if Env[i, j] == 1:  # Wall\n",
    "                    continue\n",
    "\n",
    "                if (i, j) == GOAL_STATE:  # Force goal state value to 0\n",
    "                    V_new[i, j] = 0\n",
    "                    continue\n",
    "\n",
    "                curr_v = V[i, j]\n",
    "                action = policy[i, j]\n",
    "                curr_state = (i, j)\n",
    "\n",
    "                V_new[i, j] = calculate_state_value(Env, curr_state, action, p, V, gamma)\n",
    "                delta = max(delta, abs(curr_v - V_new[i, j]))\n",
    "\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "def policy_improvement(Env, V, p, gamma):\n",
    "    \"\"\"\n",
    "    Function to improve the policy\n",
    "    \"\"\"\n",
    "    policy = np.full((Env.shape[0], Env.shape[1]), '', dtype=object)\n",
    "\n",
    "    for i in range(Env.shape[0]):\n",
    "        for j in range(Env.shape[1]):\n",
    "            if Env[i, j] == 1:  # Wall\n",
    "                continue\n",
    "\n",
    "            curr_state = (i, j)\n",
    "            possible_actions = ['up', 'down', 'left', 'right']\n",
    "            action_values = []\n",
    "\n",
    "            # Calculate value of each action\n",
    "            for action in possible_actions:\n",
    "                v = calculate_state_value(Env, curr_state, action, p, V, gamma)\n",
    "                action_values.append(v)\n",
    "\n",
    "            # Get best action and update policy\n",
    "            best_action = possible_actions[np.argmax(action_values)]\n",
    "            policy[i, j] = best_action\n",
    "\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(Env, policy, p, gamma, theta):\n",
    "    \"\"\"\n",
    "    Function to perform policy iteration\n",
    "    \"\"\"\n",
    "    curr_policy = policy.copy()\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "\n",
    "        V = policy_evaluation(Env, curr_policy, p, gamma, theta)\n",
    "        updated_policy = policy_improvement(Env, V, p, gamma)\n",
    "\n",
    "        if np.array_equal(updated_policy, curr_policy):\n",
    "            break\n",
    "\n",
    "        curr_policy = updated_policy.copy()\n",
    "\n",
    "    return curr_policy, V, iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Iteration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(Env, p, gamma, theta):\n",
    "    V = np.zeros(Env.shape)\n",
    "    iterations = 0\n",
    "\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        delta = 0\n",
    "        V_new = np.copy(V)\n",
    "\n",
    "        for i in range(Env.shape[0]):\n",
    "            for j in range(Env.shape[1]):\n",
    "                if Env[i, j] == 1:  # Skip walls\n",
    "                    continue\n",
    "\n",
    "                if (i, j) == GOAL_STATE:  # Force goal state value to 0\n",
    "                    V_new[i, j] = 0\n",
    "                    continue\n",
    "\n",
    "                curr_state = (i, j)\n",
    "                curr_v = V[i, j]\n",
    "                possible_actions = ['up', 'down', 'left', 'right']\n",
    "                action_values = []\n",
    "\n",
    "                # Calculate value of each action\n",
    "                for action in possible_actions:\n",
    "                    v = calculate_state_value(Env, curr_state, action, p, V, gamma)\n",
    "                    action_values.append(v)\n",
    "                V_new[i, j] = max(action_values)\n",
    "\n",
    "                delta = max(delta, abs(V_new[i, j] - curr_v))\n",
    "\n",
    "        V = V_new\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Compute final policy\n",
    "    policy = policy_improvement(Env, V, p, gamma)\n",
    "\n",
    "    return policy, V, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(policy_method, probabilities, gammas, theta):\n",
    "    Env = init_env()\n",
    "    for i in range(len(probabilities)):\n",
    "        p = probabilities[i]\n",
    "        gamma = gammas[i]\n",
    "\n",
    "        if policy_method == \"policy iteration\":\n",
    "            # Policy Initialization\n",
    "            policy = np.full((Env.shape[0], Env.shape[1]), 'left', dtype=object)\n",
    "\n",
    "            # Policy Iteration\n",
    "            optimal_policy, V, iteration = policy_iteration(Env, policy, p, gamma, theta)\n",
    "        \n",
    "        elif policy_method == \"value iteration\":\n",
    "            optimal_policy, V, iteration = value_iteration(Env, p, gamma, theta)\n",
    "\n",
    "        print(f\"\\n{policy_method}\")\n",
    "        print(f\"Number of Iterations for p={p} and gamma={gamma}: {iteration}\")\n",
    "        # Plotting the optimal policy\n",
    "        plot_policy(Env=Env, title=f\"Optimal Policy for p={p} and gamma={gamma}\", policy=optimal_policy)\n",
    "\n",
    "\n",
    "        # Plotting the state values\n",
    "        plot_env(Env, title=f\"State Values for p={p} and gamma={gamma}\", annot_matrix=np.round(V, 2))\n",
    "\n",
    "        # Plotting the optimal path\n",
    "        start = (15, 4)\n",
    "        end = (3, 13)\n",
    "        optimal_path = get_optimal_path(Env, start, end, optimal_policy)\n",
    "        plot_optimal_path(Env=Env, title=f\"Optimal Path for p={p} and gamma={gamma}\", path=optimal_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [0.025, 0.4, 0.025]\n",
    "gammas = [0.99, 0.99, 0.5]\n",
    "theta = 0.01\n",
    "\n",
    "simulate(\"policy iteration\", probabilities, gammas, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate(\"value iteration\", probabilities, gammas, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "NUM_STATES = 16\n",
    "NUM_ACTIONS = 5\n",
    "NUM_GENES = 4\n",
    "ACTIONS = np.vstack(([0, 0, 0, 0], np.eye(NUM_GENES, dtype=int)))\n",
    "C = np.array([[0, 0, -1, 0],\n",
    "              [1, 0, -1, -1],\n",
    "              [0, 1, 0, 0],\n",
    "              [-1, 1, 1, 0]])\n",
    "\n",
    "def get_policy_with_action(action):\n",
    "    \"\"\"Returns a policy with all actions as given action.\"\"\"\n",
    "    return np.full(NUM_STATES, action, dtype=int)\n",
    "\n",
    "def xor_mod2(vec1, vec2):\n",
    "    \"\"\"Performs component-wise XOR (mod 2 addition).\"\"\"\n",
    "    return (vec1 + vec2) % 2\n",
    "\n",
    "def get_state_from_idx(idx):\n",
    "    \"\"\"Converts an index to a binary state.\"\"\"\n",
    "    return np.array(list(map(int, format(idx, '04b'))))\n",
    "\n",
    "def v_bar(vec):\n",
    "    output = np.zeros(len(vec))\n",
    "    for i, elem in enumerate(vec):\n",
    "        if elem > 0:\n",
    "            output[i] = 1\n",
    "        else:\n",
    "            output[i] = 0\n",
    "    \n",
    "    return output\n",
    "\n",
    "def abs_sum(vec):\n",
    "    return np.sum(np.abs(vec))\n",
    "\n",
    "def compute_N(p):\n",
    "    \"\"\"Returns 1 with probability p and 0 with probability 1-p.\"\"\"\n",
    "    N = []\n",
    "    for _ in range(NUM_GENES):\n",
    "        N.append(int(np.random.rand() < p))\n",
    "    return N\n",
    "\n",
    "def compute_next_state(p, state, action):\n",
    "    \"\"\"Computes the transition probability matrices for a given action and state.\"\"\"\n",
    "    N = compute_N(p)\n",
    "    S_NEW = xor_mod2(xor_mod2(v_bar(C @ state), action), N)\n",
    "\n",
    "    return S_NEW\n",
    "\n",
    "def compute_transition_matrix(p, action):\n",
    "    \"\"\"Computes the transition probability matrices for a given action.\"\"\"\n",
    "    M = np.zeros((NUM_STATES, NUM_STATES))\n",
    "    for i in range(NUM_STATES):\n",
    "        si = get_state_from_idx(i)\n",
    "        Csi = v_bar(C @ si)\n",
    "        Csia = xor_mod2(Csi, action)\n",
    "\n",
    "        for j in range(NUM_STATES):\n",
    "            sj = get_state_from_idx(j)\n",
    "            exp = abs_sum(sj - Csia)\n",
    "            M[i, j] = (p ** exp) * ((1 - p) ** (4 - exp))\n",
    "    \n",
    "    return M\n",
    "\n",
    "def compute_reward_function(action):\n",
    "    \"\"\"Defines the reward function.\"\"\"\n",
    "    R = np.zeros((NUM_STATES, NUM_STATES))\n",
    "    for i in range(NUM_STATES):\n",
    "        for j in range(NUM_STATES):\n",
    "            next_state = get_state_from_idx(j)\n",
    "            R[i, j] = 5 * sum(next_state) - abs_sum(action)\n",
    "    return R\n",
    "\n",
    "def init_P_R(p):\n",
    "    P = np.zeros((NUM_ACTIONS, NUM_STATES, NUM_STATES))\n",
    "    for a in range(NUM_ACTIONS):\n",
    "        P[a] = compute_transition_matrix(p, ACTIONS[a])\n",
    "\n",
    "    R = np.zeros((NUM_ACTIONS, NUM_STATES, NUM_STATES))\n",
    "    for a in range(NUM_ACTIONS):\n",
    "        R[a] = compute_reward_function(ACTIONS[a])\n",
    "    \n",
    "    return P, R\n",
    "\n",
    "def compute_average_activation(policy, P, num_episodes, episode_length):\n",
    "    \"\"\"Computes the average activation rate over multiple episodes.\"\"\"\n",
    "    avg_activation = 0\n",
    "    for _ in range(num_episodes):\n",
    "        activation_sum = 0\n",
    "        curr_state_idx = np.random.choice(NUM_STATES)\n",
    "        for _ in range(episode_length):\n",
    "            state = get_state_from_idx(curr_state_idx)\n",
    "            activation_sum += np.sum(state)\n",
    "            action = policy[curr_state_idx]\n",
    "            transition_probs = P[action][curr_state_idx]\n",
    "            next_state_idx = np.random.choice(NUM_STATES, p=transition_probs)\n",
    "            curr_state_idx = next_state_idx\n",
    "        avg_activation += activation_sum / episode_length\n",
    "    avg_activation_rate = avg_activation / num_episodes\n",
    "    return avg_activation_rate\n",
    "\n",
    "def get_printable_policy(policy):\n",
    "    \"\"\"Prints the optimal policy.\"\"\"\n",
    "    output_policy = []\n",
    "    for action in policy:\n",
    "        output_policy.append(f\"a{action+1}\")\n",
    "    \n",
    "    return output_policy\n",
    "\n",
    "def matrix_form_value_iteration(P, R, gamma, theta):\n",
    "    \"\"\"Performs Value Iteration in matrix form to compute the optimal policy.\"\"\"\n",
    "    V = np.zeros(NUM_STATES)\n",
    "    policy = np.zeros(NUM_STATES, dtype=int)\n",
    "    iterations = 0\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        iterations += 1\n",
    "        for s in range(NUM_STATES):\n",
    "            v = V[s]\n",
    "            action_values = [np.sum(P[a, s, :] * (R[a, s, :] + (gamma * V))) for a in range(NUM_ACTIONS)]\n",
    "            V[s] = max(action_values)\n",
    "            policy[s] = np.argmax(action_values)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return policy, V, iterations\n",
    "\n",
    "def matrix_form_policy_iteration(P, R, initial_policy, gamma):\n",
    "    \"\"\"Performs Policy Iteration in matrix form to compute the optimal policy.\"\"\"\n",
    "    V = np.zeros(NUM_STATES)\n",
    "    policy = initial_policy\n",
    "    iterations = 0\n",
    "\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        V = np.zeros(NUM_STATES)\n",
    "        policy_stable = True\n",
    "        for s in range(NUM_STATES):\n",
    "            action = policy[s]\n",
    "            V[s] = np.sum(P[action, s, :] * (R[action, s, :] + (gamma * V)))\n",
    "        \n",
    "        for s in range(NUM_STATES):\n",
    "            old_action = policy[s]\n",
    "            action_values = [np.sum(P[a, s, :] * (R[a, s, :] + (gamma * V))) for a in range(NUM_ACTIONS)]\n",
    "            policy[s] = np.argmax(action_values)\n",
    "            if old_action != policy[s]:\n",
    "                policy_stable = False\n",
    "        \n",
    "        if policy_stable:\n",
    "            break\n",
    "    \n",
    "    return policy, V, iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=100\n",
    "episode_length=200\n",
    "gamma = 0.95\n",
    "theta = 0.01\n",
    "p_values = [0.04, 0.15, 0.48]\n",
    "no_control_policy = get_policy_with_action(0)\n",
    "\n",
    "for p in p_values:\n",
    "    P, R = init_P_R(p)\n",
    "    policy, V, iterations = matrix_form_value_iteration(P, R, gamma, theta)\n",
    "    print(f\"p = {p}\")\n",
    "    print(f\"Optimal policy: {get_printable_policy(policy)}\")\n",
    "    print(f\"Optimal value function: {np.round(V, 4)}\")\n",
    "    print(f\"Number of iterations: {iterations}\")\n",
    "\n",
    "    avg_activation = compute_average_activation(policy, P, num_episodes, episode_length)\n",
    "    avg_activation_ncp = compute_average_activation(no_control_policy, P, num_episodes, episode_length)\n",
    "    print(f\"Average activation rate: {avg_activation}\")\n",
    "    print(f\"Average activation rate with no control policy: {avg_activation_ncp}\")\n",
    "    print(\"---------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nPolicy Iteration\")\n",
    "print(\"p = 0.05\")\n",
    "P, R = init_P_R(0.05)\n",
    "policy, V, iterations = matrix_form_policy_iteration(P, R, no_control_policy, gamma)\n",
    "print(f\"Optimal policy: {get_printable_policy(policy)}\")\n",
    "print(f\"Optimal value function: {np.round(V, 4)}\")\n",
    "print(f\"Number of iterations: {iterations}\")\n",
    "\n",
    "avg_activation = compute_average_activation(policy, P, num_episodes, episode_length)\n",
    "avg_activation_ncp = compute_average_activation(no_control_policy, P, num_episodes, episode_length)\n",
    "print(f\"Average activation rate: {avg_activation}\")\n",
    "print(f\"Average activation rate with no control policy: {avg_activation_ncp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
